# ============================================================================
# RAG-Based Hindi QA System — Environment Configuration
# ============================================================================
# IMPORTANT: Copy this file to .env and update all values before deployment
# NEVER commit the actual .env file to version control
# ============================================================================

# ============================================================================
# APPLICATION
# ============================================================================
APP_ENV=production                              # production | staging | development
APP_DEBUG=false
APP_LOG_LEVEL=INFO                              # DEBUG | INFO | WARNING | ERROR
APP_SECRET_KEY=<generate-with-$(openssl rand -hex 32)>  # Random 256-bit hex for encryption

# ============================================================================
# POSTGRES (Main Application Database)
# ============================================================================
POSTGRES_HOST=postgres
POSTGRES_PORT=5432
POSTGRES_DB=ragqa
POSTGRES_USER=ragqa_user
POSTGRES_PASSWORD=<generate-strong-password>    # Minimum 16 characters

# ============================================================================
# POSTGRES (Langfuse Observability Database)
# ============================================================================
LANGFUSE_PG_HOST=langfuse-postgres
LANGFUSE_PG_PORT=5433
LANGFUSE_PG_DB=langfuse
LANGFUSE_PG_USER=langfuse_user
LANGFUSE_PG_PASSWORD=<generate-strong-password>

# ============================================================================
# REDIS
# ============================================================================
REDIS_HOST=redis
REDIS_PORT=6379
REDIS_PASSWORD=<generate-strong-password>      # For Redis AUTH
REDIS_DB_CACHE=0                                # Query result caching
REDIS_DB_RATE_LIMIT=1                           # Rate limiter state
REDIS_DB_SESSION=2                              # User session storage
REDIS_DB_TRANSLATION=3                          # Translation result cache

# ============================================================================
# MILVUS (Vector Database)
# ============================================================================
MILVUS_HOST=milvus
MILVUS_PORT=19530
MILVUS_COLLECTION_TEXT=ministry_text            # Text embedding collection
MILVUS_COLLECTION_IMAGE=ministry_images         # Vision embedding collection

# ============================================================================
# MINIO (Object Storage)
# ============================================================================
MINIO_ENDPOINT=minio:9000
MINIO_ACCESS_KEY=<generate-minio-access-key>   # Like AWS access key
MINIO_SECRET_KEY=<generate-minio-secret-key>   # Like AWS secret key
MINIO_BUCKET_DOCUMENTS=documents                # Raw and processed documents
MINIO_BUCKET_MODELS=models                      # Fine-tuned model weights
MINIO_BUCKET_BACKUPS=backups                    # Backup archives
MINIO_USE_SSL=false                             # Internal traffic (no TLS needed)
# Total MinIO storage: 10TB (configured in docker-compose.yml volumes)

# ============================================================================
# JWT (JSON Web Tokens)
# ============================================================================
JWT_SECRET_KEY=<generate-with-$(openssl rand -hex 32)>  # Different from APP_SECRET_KEY
JWT_ALGORITHM=HS256
JWT_ACCESS_TOKEN_EXPIRE_MINUTES=60              # Access token validity
JWT_REFRESH_TOKEN_EXPIRE_DAYS=7                 # Refresh token validity

# ============================================================================
# LLM SERVICE (vLLM with GPU acceleration)
# ============================================================================
LLM_SERVICE_URL=http://llm-service:8002
LLM_API_KEY=sk-ragqa-llm-<random>              # OpenAI-compatible API key
LLM_MODEL_STANDARD=meta-llama/Llama-3.1-8B-Instruct-AWQ        # Standard QA model
LLM_MODEL_LONGCTX=mistralai/Mistral-NeMo-Instruct-2407-AWQ    # Long-context model
LLM_MODEL_MULTIMODAL=google/gemma-3-12b-it-awq                 # Multimodal model
LLM_GPU_MEMORY_UTILIZATION=0.85                 # GPU memory usage ratio
LLM_MAX_MODEL_LEN_STANDARD=8192                 # Token limit for standard model
LLM_MAX_MODEL_LEN_LONGCTX=131072                # Token limit for long-context model
LLM_MAX_MODEL_LEN_MULTIMODAL=8192               # Token limit for multimodal model

# ============================================================================
# RAG SERVICE (Retrieval-Augmented Generation)
# ============================================================================
RAG_SERVICE_URL=http://rag-service:8001
RAG_EMBEDDING_MODEL=BAAI/bge-m3                 # Multilingual dense+sparse embeddings
RAG_VISION_EMBEDDING_MODEL=google/siglip-so400m-patch14-384  # Vision embeddings
RAG_CHUNK_SIZE=512                              # Document chunk size (tokens)
RAG_CHUNK_OVERLAP=64                            # Overlap between chunks
RAG_TOP_K=10                                    # Top-K results from retrieval
RAG_RERANK_TOP_K=5                              # Rerank to top-5 results
RAG_CONFIDENCE_THRESHOLD=0.65                   # Below this → use fallback response
RAG_CACHE_TTL_SECONDS=3600                      # Cache time-to-live (1 hour)

# ============================================================================
# SPEECH SERVICE (STT + TTS)
# ============================================================================
SPEECH_SERVICE_URL=http://speech-service:8003
SPEECH_STT_MODEL=ai4bharat/indicconformer-hi-en      # Speech-to-text model
SPEECH_TTS_HINDI_MODEL=ai4bharat/indic-tts-hindi    # Hindi text-to-speech
SPEECH_TTS_ENGLISH_MODEL=coqui/tts-english           # English text-to-speech
SPEECH_SAMPLE_RATE=16000                             # Audio sample rate (Hz)

# ============================================================================
# TRANSLATION SERVICE (IndicTrans2 for Indic languages)
# ============================================================================
TRANSLATION_SERVICE_URL=http://translation-service:8004
TRANSLATION_MODEL=ai4bharat/indictrans2-indic-en-1B  # Indic↔English translation
TRANSLATION_CACHE_TTL_SECONDS=86400                   # Cache for 24 hours

# ============================================================================
# OCR SERVICE (Optical Character Recognition)
# ============================================================================
OCR_SERVICE_URL=http://ocr-service:8005
OCR_TESSERACT_LANG=hin+eng                      # Tesseract language codes
OCR_EASYOCR_LANGS=hi,en                         # EasyOCR language codes

# ============================================================================
# DATA INGESTION (Web scraping + document parsing)
# ============================================================================
INGESTION_SERVICE_URL=http://data-ingestion:8006
INGESTION_SCRAPE_INTERVAL_HOURS=24              # How often to re-scrape sites
INGESTION_MAX_CONCURRENT_SPIDERS=4              # Parallel spider instances
INGESTION_RESPECT_ROBOTS_TXT=true               # Honor robots.txt directives

# ============================================================================
# MODEL TRAINING (LoRA fine-tuning)
# ============================================================================
TRAINING_SERVICE_URL=http://model-training:8007
TRAINING_LORA_RANK=16                           # LoRA rank
TRAINING_LORA_ALPHA=32                          # LoRA scaling factor
TRAINING_LEARNING_RATE=2e-4                     # Training learning rate
TRAINING_EPOCHS=3                               # Training epochs
TRAINING_BATCH_SIZE=4                           # Batch size per GPU

# ============================================================================
# LANGFUSE (LLM Observability & Tracing)
# ============================================================================
LANGFUSE_HOST=http://langfuse:3001
LANGFUSE_PUBLIC_KEY=<generate-langfuse-public-key>   # API public key
LANGFUSE_SECRET_KEY=<generate-langfuse-secret-key>   # API secret key
LANGFUSE_NEXTAUTH_SECRET=<generate-with-$(openssl rand -hex 32)>  # NextAuth secret
LANGFUSE_SALT=<generate-with-$(openssl rand -hex 32)>             # Password salt

# ============================================================================
# SESSION MANAGEMENT
# ============================================================================
SESSION_IDLE_TIMEOUT_SECONDS=1800               # Session timeout (30 minutes)
SESSION_MAX_TURNS=50                            # Max conversation turns
SESSION_CONTEXT_WINDOW_TOKENS=4096              # Max context tokens to retain

# ============================================================================
# DATA RETENTION (in days)
# ============================================================================
RETENTION_CONVERSATIONS_DAYS=90                 # Conversation history retention
RETENTION_FEEDBACK_DAYS=365                     # User feedback retention (1 year)
RETENTION_AUDIT_LOG_DAYS=730                    # Audit log retention (2 years)
RETENTION_ANALYTICS_DAYS=365                    # Analytics data retention (1 year)
RETENTION_TRANSLATION_CACHE_DAYS=30             # Translation cache retention

# ============================================================================
# RATE LIMITING (requests per minute, per user role)
# ============================================================================
RATE_LIMIT_ADMIN=120                            # Admin rate limit
RATE_LIMIT_EDITOR=90                            # Content editor rate limit
RATE_LIMIT_VIEWER=30                            # Read-only viewer limit
RATE_LIMIT_API_CONSUMER=60                      # External API consumer limit

# ============================================================================
# NGINX / TLS / CORS
# ============================================================================
NGINX_DOMAIN=culture.gov.in                     # Primary domain
NGINX_SSL_CERT_PATH=/etc/nginx/ssl/cert.pem    # TLS certificate path
NGINX_SSL_KEY_PATH=/etc/nginx/ssl/key.pem      # TLS key path
CORS_ALLOWED_ORIGINS=https://culture.gov.in,https://www.culture.gov.in  # CORS whitelist

# ============================================================================
# GRAFANA
# ============================================================================
GRAFANA_ADMIN_USER=admin                        # Grafana admin username
GRAFANA_ADMIN_PASSWORD=<generate-strong-password>  # Grafana admin password

# ============================================================================
# GPU REQUIREMENTS (Document supported hardware)
# ============================================================================
# Required NVIDIA Driver:  >= 535
#   Install: https://www.nvidia.com/Download/driverDetails.aspx
#   Verify:  nvidia-smi
#
# Required CUDA Toolkit:   >= 12.1
#   Install: https://developer.nvidia.com/cuda-12-1-0-download-archive
#   Verify:  nvcc --version
#
# Required nvidia-container-toolkit:
#   Install: https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html
#   Verify:  docker run --rm --runtime=nvidia nvidia/cuda:12.1.0-runtime-ubuntu22.04 nvidia-smi
#
# GPU Assignment (in docker-compose.yml):
#   - llm-service:         GPU 0 (primary LLM inference)
#   - speech-service:      GPU 1 (STT/TTS)
#   - translation-service: GPU 2 (Translation)
#   - model-training:      GPU 3 (Fine-tuning)
#   - dcgm-exporter:       All GPUs (monitoring)
#
# Minimum GPU Memory Required per service:
#   - llm-service:         20GB (for Llama 3.1 8B + Mistral NeMo 12B)
#   - speech-service:      8GB
#   - translation-service: 8GB
#   - model-training:      16GB
# Total: 52GB minimum (recommend 4x A100 40GB or equivalent)
#
# MultiGPU Setup Example (NVLink):
#   If using NVLink interconnect (A100s), add to docker-compose.yml:
#   llm-service:
#     deploy:
#       resources:
#         reservations:
#           devices:
#             - driver: nvidia
#               count: 2                    # Use 2 GPUs
#               capabilities: [gpu]

# ============================================================================
# COMPLIANCE & DATA CENTER REQUIREMENTS
# ============================================================================
# IMPORTANT: This application is designed to run on:
# - NIC/MeitY-empanelled Data Centres in India
# - Compliance: MeitY Cloud guidelines, Indian Data Protection Act 2023
# - Data residency: All data must remain within Indian territory
# - Backup: Maintained within Indian data centers
#
# Before deployment, verify:
# 1. Host datacenter is NIC/MeitY empanelled
# 2. Network connectivity follows MeitY standards
# 3. Firewall rules allow only Ministry of Culture traffic
# 4. Backup storage is in-country
# 5. All services are updated to latest security patches
